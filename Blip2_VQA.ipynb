{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5623ed-37a1-4a52-b889-163fb2a7be8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-24.2\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-o4s95vp8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-o4s95vp8\n",
      "  Resolved https://github.com/huggingface/transformers to commit c63a3d0f1791e018de447ac570fc7029d1ea19bd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2023.7.22)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9553576 sha256=698855451cbfd2b6cc0f9d73c09c17527b2d493230c5ba5fc815f5db5c825ad0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-avg6hh0b/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.1\n",
      "    Uninstalling transformers-4.44.1:\n",
      "      Successfully uninstalled transformers-4.44.1\n",
      "Successfully installed transformers-4.45.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets \n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f711de80-a808-4795-923c-cd1cbaff0e79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 17:14:07.687363: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-17 17:14:07.730918: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-17 17:14:07.730987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-17 17:14:07.732136: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-17 17:14:07.739160: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-17 17:14:08.770512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78ab3a94cff40fba3dc7a51aa900e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2fac1f3250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import torch\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Load the BLIP2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# # Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2059d3b6-beac-45e6-9e55-bf0f54651e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "\n",
    "class ChartQADataset(Dataset):\n",
    "    \"\"\"ChartQA dataset.\"\"\"\n",
    "    def __init__(self, json_file, image_folder, processor, image_size=(512, 512)):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_size = image_size\n",
    "        self.processor = processor\n",
    "\n",
    "        # Load JSON data\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.dataset = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_folder, data['imgname'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize image\n",
    "        image = image.resize(self.image_size)\n",
    "\n",
    "        # Get text data\n",
    "        query = \"Question: \" + data['query']\n",
    "        label = data['label']\n",
    "        \n",
    "        # Process image and text\n",
    "        encodings = self.processor(images=image, text=query, padding='max_length', truncation=True, max_length=100, return_tensors=\"pt\")\n",
    "        \n",
    "        # Process labels\n",
    "        labels = self.processor.tokenizer.encode(label, max_length=6, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        \n",
    "        encodings[\"labels\"] = labels.squeeze()\n",
    "        \n",
    "        return encodings\n",
    "\n",
    "# File paths\n",
    "train_json_file = 'chartqa/train/train_merged.json'\n",
    "val_json_file = 'chartqa/val/val_merged.json'\n",
    "test_json_file = 'chartqa/test/test_merged.json'\n",
    "\n",
    "train_image_folder = 'chartqa/train/png'\n",
    "val_image_folder = 'chartqa/val/png'\n",
    "test_image_folder = 'chartqa/test/png'\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ChartQADataset(train_json_file, train_image_folder, processor)\n",
    "val_dataset = ChartQADataset(val_json_file, val_image_folder, processor)\n",
    "test_dataset = ChartQADataset(test_json_file, test_image_folder, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01395326-39d0-44ac-a005-a1e3fea2d9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dataloaders and other components\n",
    "batch_size = 3  # Reduced batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21cc5a42-ad2e-4a69-bfb9-cf98b9261d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.15, patience=2, verbose=True)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 25\n",
    "patience = 3\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Ensure gradients are zeroed at the start of each epoch\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc='Training batch: ...'):\n",
    "        input_ids = batch.pop('input_ids').squeeze(1).to(device)\n",
    "        pixel_values = batch.pop('pixel_values').squeeze(1).to(device)\n",
    "        attention_mask = batch.pop('attention_mask').squeeze(1).to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            labels=labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()  # Perform optimizer step\n",
    "        optimizer.zero_grad()  # Reset gradients after step\n",
    "\n",
    "        # Optionally, clear cache to manage memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_accuracy = 0  # Initialize accuracy metric\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc='Validating batch: ...'):\n",
    "            input_ids = batch.pop('input_ids').squeeze(1).to(device)\n",
    "            pixel_values = batch.pop('pixel_values').squeeze(1).to(device)\n",
    "            attention_mask = batch.pop('attention_mask').squeeze(1).to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                pixel_values=pixel_values,\n",
    "                                labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy (if relevant for your task)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            eval_accuracy += (preds == labels).sum().item() / len(labels)\n",
    "\n",
    "    avg_val_loss = eval_loss / len(valid_dataloader)\n",
    "    avg_val_accuracy = eval_accuracy / len(valid_dataloader)  # Average accuracy\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    tracking_information.append((avg_train_loss, avg_val_loss, avg_val_accuracy, optimizer.param_groups[0][\"lr\"]))\n",
    "    print(f\"Epoch: {epoch+1} - Training loss: {avg_train_loss:.4f} - Eval Loss: {avg_val_loss:.4f} - Accuracy: {avg_val_accuracy:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "   \n",
    "    # Save the best model\n",
    "    try:\n",
    "        if avg_val_loss < min_eval_loss:\n",
    "            torch.save(model.state_dict(), 'vqa_best_model.pth') \n",
    "            print(\"Saved model to vqa_best_model.pth\")\n",
    "            min_eval_loss = avg_val_loss\n",
    "            early_stopping_hook = 0\n",
    "        else:\n",
    "            early_stopping_hook += 1\n",
    "            if early_stopping_hook > patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        # Save the last model\n",
    "        torch.save(model.state_dict(), 'vqa_last_model.pth')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the model: {e}\")\n",
    "\n",
    "# Save tracking information\n",
    "with open(\"chartqa_tracking_information.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tracking_information, f)\n",
    "print(\"The fine-tuning process has completed!\")\n",
    "\n",
    "epochs_completed = len(train_losses)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Losses\n",
    "plt.plot(range(1, epochs_completed + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, epochs_completed + 1), val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_plot.png')  # Save the plot as a .png file\n",
    "plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9aa7688-2ac3-4b93-985e-044f59da533d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nltk\n",
      "  Downloading nltk-3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting word2number\n",
      "  Using cached word2number-1.1-py3-none-any.whl\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ip (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: word2number, nltk\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.9 word2number-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk word2number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cf53ea6-27da-4131-b2be-6d0aaf45cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Evaluating batch:: 100%|██████████| 834/834 [07:00<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: How many stores did Saint Laurent operate in Western Europe in 2020?\n",
      "Predicted Label: 71\n",
      "Actual Label: 47\n",
      "\n",
      "Input: Question: In what year did online sales make up 6.8 percent of retail sales of jewelry, watches and accessories in Germany?\n",
      "Predicted Label: 2013\n",
      "Actual Label: 2013\n",
      "\n",
      "Input: Question: What percentage of the retail sales of jewelry, watches and accessories in Germany were online in 2013?\n",
      "Predicted Label: 11.\n",
      "Actual Label: 6.8\n",
      "\n",
      "Input: Question: What is the predicted increase in online sales of jewelry, watches and accessories in Germany by 2018?\n",
      "Predicted Label: 11\n",
      "Actual Label: 11\n",
      "\n",
      "Input: Question: How many companies were in Hungary's insurance market in 2013?\n",
      "Predicted Label: 28\n",
      "Actual Label: 36\n",
      "\n",
      "Input: Question: How many companies were in Hungary's insurance market in 2019?\n",
      "Predicted Label: 28\n",
      "Actual Label: 23\n",
      "\n",
      "Input: Question: How many drone strikes did the U.S. carry out in Somalia in 2019?\n",
      "Predicted Label: 12\n",
      "Actual Label: 63\n",
      "\n",
      "Input: Question: In what year did both genders achieve their highest pass rates?\n",
      "Predicted Label: 2019\n",
      "Actual Label: 2011\n",
      "\n",
      "Input: Question: What percentage of female students achieved a C/4 grade or higher in the United Kingdom in 2020?\n",
      "Predicted Label: 79.4\n",
      "Actual Label: 80.2\n",
      "\n",
      "Input: Question: What percentage of male students achieved a C/4 grade or higher in 2020?\n",
      "Predicted Label: 79.4\n",
      "Actual Label: 72.3\n",
      "\n",
      "Input: Question: Who is predicted to have the highest gross profit margin?\n",
      "Predicted Label: T-\n",
      "Actual Label: Ted Baker\n",
      "\n",
      "Input: Question: How many of the civilians killed in the first six months of 2021 were Black?\n",
      "Predicted Label: 85\n",
      "Actual Label: 74\n",
      "\n",
      "Input: Question: How many journalists were murdered in 2020?\n",
      "Predicted Label: 11\n",
      "Actual Label: 32\n",
      "\n",
      "Input: Question: In what year was the number of journalists killed similar to the number in 2020?\n",
      "Predicted Label: 2019\n",
      "Actual Label: 2019\n",
      "\n",
      "Input: Question: What new generation of mobile technology is forecast to gain market revenue in the coming years?\n",
      "Predicted Label: 5-\n",
      "Actual Label: 5G\n",
      "\n",
      "Test Loss: 1.8720, Accuracy: 46.30%, Relaxed Correctness: 23.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import difflib\n",
    "from nltk.corpus import wordnet\n",
    "from word2number import w2n\n",
    "import nltk\n",
    "\n",
    "# Download WordNet data for nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return synonyms\n",
    "\n",
    "def word_to_num(word):\n",
    "    try:\n",
    "        return w2n.word_to_num(word)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def relaxed_correctness(prediction, ground_truth, num_threshold=0.05, text_similarity_threshold=0.8):\n",
    "    prediction = prediction.lower().strip()\n",
    "    ground_truth = ground_truth.lower().strip()\n",
    "\n",
    "    # Try to handle numbers\n",
    "    pred_num = word_to_num(prediction)\n",
    "    gt_num = word_to_num(ground_truth)\n",
    "\n",
    "    if pred_num is not None and gt_num is not None:\n",
    "        lower_bound = gt_num * (1 - num_threshold)\n",
    "        upper_bound = gt_num * (1 + num_threshold)\n",
    "        if lower_bound <= pred_num <= upper_bound:\n",
    "            return 1.0\n",
    "\n",
    "    if pred_num is not None and ground_truth.isdigit():\n",
    "        lower_bound = int(ground_truth) * (1 - num_threshold)\n",
    "        upper_bound = int(ground_truth) * (1 + num_threshold)\n",
    "        if lower_bound <= pred_num <= upper_bound:\n",
    "            return 1.0\n",
    "\n",
    "    if gt_num is not None and prediction.isdigit():\n",
    "        lower_bound = gt_num * (1 - num_threshold)\n",
    "        upper_bound = gt_num * (1 + num_threshold)\n",
    "        if lower_bound <= int(prediction) <= upper_bound:\n",
    "            return 1.0\n",
    "\n",
    "    # Exact match\n",
    "    if prediction == ground_truth:\n",
    "        return 1.0\n",
    "\n",
    "    # Synonym match\n",
    "    prediction_synonyms = get_synonyms(prediction)\n",
    "    ground_truth_synonyms = get_synonyms(ground_truth)\n",
    "    if prediction in ground_truth_synonyms or ground_truth in prediction_synonyms:\n",
    "        return 1.0\n",
    "\n",
    "    # Approximate string matching\n",
    "    similarity = difflib.SequenceMatcher(None, prediction, ground_truth).ratio()\n",
    "    if similarity > text_similarity_threshold:  # Threshold for relaxed match\n",
    "        return similarity\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "def evaluate(model, loader, processor, device, num_examples=15):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_relaxed_correctness_scores = []\n",
    "\n",
    "    example_inputs = []\n",
    "    example_predictions = []\n",
    "    example_ground_truths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating batch:'):\n",
    "            input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "            pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            model.to(device)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "            if len(example_inputs) < num_examples:\n",
    "                example_inputs.extend(input_ids.cpu().numpy())\n",
    "                example_predictions.extend(preds.cpu().numpy())\n",
    "                example_ground_truths.extend(labels.cpu().numpy())\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                try:\n",
    "                    model_answer = processor.tokenizer.decode(preds[i], skip_special_tokens=True)\n",
    "                    ground_truth = processor.tokenizer.decode(labels[i], skip_special_tokens=True)\n",
    "                    correctness = relaxed_correctness(model_answer, ground_truth)\n",
    "                    all_relaxed_correctness_scores.append(correctness)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating correctness for sample {i}: {e}\")\n",
    "                    all_relaxed_correctness_scores.append(0.0)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    accuracy = np.mean([np.mean(all_labels[:, i] == all_preds[:, i]) for i in range(all_labels.shape[1])])\n",
    "    average_relaxed_correctness = np.mean(all_relaxed_correctness_scores)\n",
    "\n",
    "    for i in range(min(num_examples, len(example_inputs))):\n",
    "        input_text = processor.tokenizer.decode(example_inputs[i], skip_special_tokens=True)\n",
    "        predicted_label = processor.tokenizer.decode(example_predictions[i], skip_special_tokens=True)\n",
    "        actual_label = processor.tokenizer.decode(example_ground_truths[i], skip_special_tokens=True)\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Predicted Label: {predicted_label}\")\n",
    "        print(f\"Actual Label: {actual_label}\")\n",
    "        print()\n",
    "\n",
    "    return average_loss, accuracy, average_relaxed_correctness\n",
    "\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "model.load_state_dict(torch.load('vqa_last_model.pth'))\n",
    "\n",
    "# Final Evaluation on Test Set\n",
    "test_loss, test_accuracy, test_relaxed_correctness = evaluate(model, test_loader, processor, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy * 100:.2f}%, Relaxed Correctness: {test_relaxed_correctness * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a2b8e-3b31-4172-a2ac-d20342ebb186",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Code to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb4ec55-099c-4195-aeaf-30916b58d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 19:17:39.039387: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-20 19:17:39.090474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-20 19:17:39.090542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-20 19:17:39.092093: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-20 19:17:39.099301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-20 19:17:40.102760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b6aaa1a9874d4ab3fd23dbac7d36b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import torch\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Load the BLIP2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# # Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "# Load the fine-tuned weights\n",
    "model.load_state_dict(torch.load('vqa_last_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Example question and image\n",
    "question = \"In what year did online sales make up 6.8 percent of retail sales of jewelry, watches and accessories in Germany?\"\n",
    "image_path = \"chartqa/test/png/multi_col_20436.png\"  # Replace with the path to your image\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "inputs = processor(text=question, images=image, return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e872663-dbae-42a0-9fda-1564d42134c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That was 2013.\n"
     ]
    }
   ],
   "source": [
    "# Check if input_ids is correctly generated\n",
    "if \"input_ids\" in inputs and inputs[\"input_ids\"].size(1) > 0:\n",
    "    # Make the prediction\n",
    "    generated_ids = model.generate(**inputs, max_length=100)  # You can set max_length to a suitable value\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"Error: input_ids were not generated correctly. Please check the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ec345-68de-47dc-9975-49a8a2295864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
